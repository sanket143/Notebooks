{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mlproject.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanket143/Notebooks/blob/master/Word2Vec%20-%20IE406.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdWAyCJCsfBg",
        "colab_type": "code",
        "outputId": "7333eeec-0273-4f5b-d1ba-f991944c7586",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import string \n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHpvuzZtsgh2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x)) \n",
        "    return e_x / e_x.sum() \n",
        "\n",
        "class word2vec(object): \n",
        "    def __init__(self):\n",
        "        self.N = 100\n",
        "        self.X_train = [] \n",
        "        self.y_train = []\n",
        "        self.window_size = 3\n",
        "        self.alpha = 0.001\n",
        "        self.words = [] \n",
        "        self.word_index = {} \n",
        "\n",
        "    def initialize(self,V,data): \n",
        "        self.V = V \n",
        "        self.W = np.random.uniform(-0.8, 0.8, (self.V, self.N)) \n",
        "        self.W1 = np.random.uniform(-0.8, 0.8, (self.N, self.V)) \n",
        "\n",
        "        self.words = data \n",
        "        for i in range(len(data)): \n",
        "            self.word_index[data[i]] = i \n",
        "\n",
        "    \n",
        "    def feed_forward(self,X): \n",
        "        self.h = np.dot(self.W.T,X).reshape(self.N,1) \n",
        "        self.u = np.dot(self.W1.T,self.h) \n",
        "        #print(self.u) \n",
        "        self.y = softmax(self.u) \n",
        "        return self.y\n",
        "        \n",
        "    def backpropagate(self,x,t): \n",
        "        e = self.y - np.asarray(t).reshape(self.V,1) \n",
        "        # e.shape is V x 1 \n",
        "        dLdW1 = np.dot(self.h,e.T) \n",
        "        #print(t)\n",
        "        X = np.array(x).reshape(self.V,1) \n",
        "        dLdW = np.dot(X, np.dot(self.W1,e).T)\n",
        "        self.W1 = self.W1 - self.alpha*dLdW1 \n",
        "        self.W = self.W - self.alpha*dLdW \n",
        "        \n",
        "    def train(self,epochs): \n",
        "        for x in range(1,epochs):\n",
        "            self.loss = 0\n",
        "            for j in range(len(self.X_train)): \n",
        "                self.feed_forward(self.X_train[j]) \n",
        "                self.backpropagate(self.X_train[j],self.y_train[j]) \n",
        "                C = 0\n",
        "                for m in range(self.V): \n",
        "                    if(self.y_train[j][m]): \n",
        "                        self.loss += -1*self.u[m][0] \n",
        "                        C += 1\n",
        "                self.loss += C*np.log(np.sum(np.exp(self.u))) \n",
        "            print(\"epoch \",x, \" loss = \",self.loss) \n",
        "            self.alpha *= 1/( (1+self.alpha*x) ) \n",
        "            \n",
        "    def predict(self,word,number_of_predictions): \n",
        "        if word in self.words: \n",
        "            index = self.word_index[word] \n",
        "            X = [0 for i in range(self.V)] \n",
        "            X[index] = 1\n",
        "            prediction = self.feed_forward(X) \n",
        "            print(prediction)\n",
        "            output = {} \n",
        "            for i in range(self.V): \n",
        "                output[prediction[i][0]] = i \n",
        "            \n",
        "            top_context_words = [] \n",
        "            for k in sorted(output,reverse=True): \n",
        "                top_context_words.append(self.words[output[k]]) \n",
        "                if(len(top_context_words)>=number_of_predictions): \n",
        "                    break\n",
        "    \n",
        "            return top_context_words \n",
        "        else: \n",
        "            print(\"Word not found in dicitonary\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBJM0NC_sjqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocessing(corpus): \n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    stop_words.add('<unk>')\n",
        "    training_data = []\n",
        "    sentences = corpus\n",
        "    for i in range(len(sentences)): \n",
        "        sentences[i] = sentences[i].strip() \n",
        "        sentence = sentences[i].split() \n",
        "        x = [word.strip(string.punctuation) for word in sentence \n",
        "                                    if word not in stop_words] \n",
        "        x = [word.lower() for word in x] \n",
        "        training_data.append(x) \n",
        "    return training_data \n",
        "    \n",
        "\n",
        "def prepare_data_for_training(sentences,w2v): \n",
        "    data = {} \n",
        "    for sentence in sentences: \n",
        "        for word in sentence: \n",
        "            if word not in data: \n",
        "                data[word] = 1\n",
        "            else: \n",
        "                data[word] += 1\n",
        "    V = len(data) \n",
        "    data = sorted(list(data.keys())) \n",
        "    vocab = {} \n",
        "    for i in range(len(data)): \n",
        "        vocab[data[i]] = i \n",
        "    \n",
        "    #for i in range(len(words)): \n",
        "    for sentence in sentences: \n",
        "        for i in range(len(sentence)): \n",
        "            center_word = [0 for x in range(V)] \n",
        "            center_word[vocab[sentence[i]]] = 1\n",
        "            context = [0 for x in range(V)] \n",
        "            \n",
        "            for j in range(i-w2v.window_size,i+w2v.window_size): \n",
        "                if i!=j and j>=0 and j<len(sentence): \n",
        "                    context[vocab[sentence[j]]] += 1\n",
        "            w2v.X_train.append(center_word) \n",
        "            w2v.y_train.append(context) \n",
        "    w2v.initialize(V,data) \n",
        "\n",
        "    return w2v.X_train,w2v.y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqnto8fvsmhr",
        "colab_type": "code",
        "outputId": "2eaed3e5-c40c-46ae-8c64-eb2a1f55ed3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/sanket143/Data/master/data2.csv\n",
        "df = pd.read_csv(\"data2.csv\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-06-07 12:31:55--  https://raw.githubusercontent.com/sanket143/Data/master/data2.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5185500 (4.9M) [text/plain]\n",
            "Saving to: ‘data2.csv.1’\n",
            "\n",
            "\rdata2.csv.1           0%[                    ]       0  --.-KB/s               \rdata2.csv.1          36%[======>             ]   1.79M  8.94MB/s               \rdata2.csv.1         100%[===================>]   4.94M  13.6MB/s    in 0.4s    \n",
            "\n",
            "2020-06-07 12:31:55 (13.6 MB/s) - ‘data2.csv.1’ saved [5185500/5185500]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jb2_60ZmsqUd",
        "colab_type": "code",
        "outputId": "0a36c8dd-fc80-4811-d8b4-b2c637e0cb2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        }
      },
      "source": [
        "corpus = df[\"title\"][0:1000]\n",
        "epochs = 100\n",
        "\n",
        "\n",
        "training_data = preprocessing(corpus) \n",
        "w2v = word2vec() \n",
        "\n",
        "prepare_data_for_training(training_data,w2v) \n",
        "w2v.train(epochs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch  1  loss =  509951.98315352725\n",
            "epoch  2  loss =  504422.7775331249\n",
            "epoch  3  loss =  502321.612200829\n",
            "epoch  4  loss =  504080.3851099931\n",
            "epoch  5  loss =  508277.9313711393\n",
            "epoch  6  loss =  515460.10509864555\n",
            "epoch  7  loss =  526798.137082982\n",
            "epoch  8  loss =  543645.2949072206\n",
            "epoch  9  loss =  567388.5397416024\n",
            "epoch  10  loss =  599526.9684703425\n",
            "epoch  11  loss =  641986.5158582601\n",
            "epoch  12  loss =  697185.53168564\n",
            "epoch  13  loss =  768071.3412893066\n",
            "epoch  14  loss =  858396.2602019691\n",
            "epoch  15  loss =  972958.4655230884\n",
            "epoch  16  loss =  1117860.400232592\n",
            "epoch  17  loss =  1301161.6807556453\n",
            "epoch  18  loss =  1533507.7767743156\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:53: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  19  loss =  inf\n",
            "epoch  20  loss =  inf\n",
            "epoch  21  loss =  inf\n",
            "epoch  22  loss =  inf\n",
            "epoch  23  loss =  inf\n",
            "epoch  24  loss =  inf\n",
            "epoch  25  loss =  inf\n",
            "epoch  26  loss =  inf\n",
            "epoch  27  loss =  inf\n",
            "epoch  28  loss =  inf\n",
            "epoch  29  loss =  inf\n",
            "epoch  30  loss =  inf\n",
            "epoch  31  loss =  inf\n",
            "epoch  32  loss =  inf\n",
            "epoch  33  loss =  inf\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n",
            "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch  34  loss =  inf\n",
            "epoch  35  loss =  inf\n",
            "epoch  36  loss =  inf\n",
            "epoch  37  loss =  inf\n",
            "epoch  38  loss =  inf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0j5zh7Bsrk_",
        "colab_type": "code",
        "outputId": "9d9364d3-93b3-4fad-f845-0c983016d254",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pd.set_option(\"max_columns\", None) # TODO this is for debug\n",
        "pd.set_option('display.width', 10000) # TODO this is for debug\n",
        "def search(word):\n",
        "  ind = w2v.word_index[word]\n",
        "  wordV = w2v.W - w2v.W[ind]\n",
        "  wordV = np.abs(wordV)\n",
        "  wordV = wordV.sum(axis=1)\n",
        "  wordV = wordV.argsort()[1:5]\n",
        "  return [w2v.words[xi] for xi in wordV]\n",
        "print(search('man'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['carla', 'skills', 'removed', 'minute']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlT_L6AVP6ex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}